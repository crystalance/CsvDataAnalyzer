# CSV 数据分析系统 - 代码实现说明

## 项目结构 & 模块职责

```
CsvDataAnalyzer/
├── app.py              # Gradio Web 界面入口
├── analyzer.py         # 核心分析器，协调 LLM 调用与代码执行
├── config.py           # 配置管理（API Key、模型选择等）
├── core/
│   ├── executor.py     # Python 代码安全执行器
│   ├── prompts.py      # 系统提示词模板
│   └── error_handler.py # 错误分类与修复建议生成
└── llm/
    ├── base.py         # LLM 抽象基类
    ├── qwen.py         # 通义千问实现
    ├── deepseek.py     # DeepSeek 实现
    └── openai_llm.py   # OpenAI 兼容接口实现
```

| 模块 | 职责 |
|------|------|
| `analyzer.py` | 编排整个分析流程：构建消息 → 调用 LLM → 执行代码 → 错误重试 → 生成解释 |
| `core/executor.py` | 沙箱执行用户代码，捕获输出/错误/图表，返回统一的 `ExecutionResult` |
| `core/error_handler.py` | 对执行错误进行分类（9种类型），生成针对性修复建议 |
| `core/prompts.py` | 管理系统提示词模板，注入 CSV 元信息 |
| `llm/*` | 多 LLM 后端支持，统一 `chat(messages)` 接口 |

---

## 核心执行流程

```
用户问题
    ↓
┌─────────────────────────────────────────────────────┐
│  1. 构建消息列表                                      │
│     - System: 提示词 + CSV 列名/类型/示例数据           │
│     - 历史对话（最近 N 条）                            │
│     - User: 当前问题                                 │
└─────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────┐
│  2. LLM 生成代码                                     │
└─────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────┐
│  3. 执行代码 (executor.py)                           │
│     - 重定向 stdout/stderr                           │
│     - 捕获图表输出                                    │
└─────────────────────────────────────────────────────┘
    ↓
    ├── 成功 → 生成解释 → 保存历史 → 返回结果
    │
    └── 失败 → 进入错误修复循环（最多 3 次）
                ↓
         ┌──────────────────────────────────────┐
         │  4. 构建错误上下文                     │
         │     - 错误类型分类 + 修复建议           │
         │     - 出错代码                        │
         │     - CSV 可用列名                    │
         │     - 最近对话历史                     │
         └──────────────────────────────────────┘
                ↓
         ┌──────────────────────────────────────┐
         │  5. 追加到消息列表                     │
         │     messages += [                    │
         │       {assistant: 上次生成的代码},     │
         │       {user: 错误上下文}               │
         │     ]                                │
         └──────────────────────────────────────┘
                ↓
         回到步骤 2，LLM 基于完整上下文重新生成
```

---

## 关键设计点

### 1. CSV 数据读取

系统通过动态路径读取 CSV，不对测试样例硬编码：

- 用户上传文件后，路径作为参数传入 `CSVAnalyzer`
- 使用 `pandas` 解析 CSV，自动提取列名、数据类型、示例数据
- 将 CSV 元信息注入系统提示词，使 LLM 了解数据结构

```python
# analyzer.py - 动态获取 CSV 信息
columns, dtypes, sample_data = self._get_csv_info()
system_prompt = PromptBuilder.build_system_prompt(csv_path, columns, dtypes, sample_data)
```

### 2. 自然语言问题输入与多轮对话

系统通过 Gradio 界面接收自然语言问题，并维护 `self.history` 列表保持对话上下文：

```python
# 每次请求注入历史对话
for item in self.history:
    messages.append({"role": "user", "content": item["question"]})
    messages.append({"role": "assistant", "content": f"```python\n{item['code']}\n```"})
```

这使用户可以连续追问，如「按地区统计销售额」→「加上时间维度」。

### 3. 基于大模型的代码生成

系统将问题 + CSV 元信息 + 历史对话组装成消息列表，调用 LLM 生成 Python 代码：

- 抽象 `BaseLLM` 接口，支持 Qwen、DeepSeek、OpenAI 等多种后端
- 系统提示词约束 LLM 只输出可执行代码，使用 `pandas` 和 `matplotlib`
- 从 LLM 响应中提取 ` ```python ` 代码块

### 4. 代码纠错机制

当代码执行失败时，系统构建错误上下文反馈给 LLM，最多重试 3 次：

```python
# 错误分类 + 修复建议
error_info = ErrorClassifier.classify(result.error)
hint = ErrorClassifier.get_hint(error_info)

# 构建错误上下文（错误信息 + 出错代码 + 可用列名 + 对话历史）
error_prompt = format_error_context(error_msg, code, columns, dtypes, history)

# 追加到消息列表，让 LLM 看到完整上下文后重新生成
messages.append({"role": "assistant", "content": response})
messages.append({"role": "user", "content": error_prompt})
```

### 5. 沙箱代码执行

`executor.py` 在隔离环境中执行 LLM 生成的代码：

- 使用独立的 `globals` 命名空间，预注入 `pandas`、`matplotlib`
- `redirect_stdout/stderr` 捕获所有输出
- 自动注入图表保存路径，替换 `plt.show()` 避免阻塞
- 统一返回 `ExecutionResult(success, output, error, figure_path)`

### 6. 基于结果的自然语言解释

代码执行成功后，系统调用 LLM 生成通俗易懂的解释：

```python
# analyzer.py - 生成解释
prompt = PromptBuilder.build_explanation_prompt(question, result.output)
explanation = self.llm.chat([{"role": "user", "content": prompt}])
```

将执行结果（数值、表格）转化为自然语言回答，便于用户理解。
